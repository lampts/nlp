{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedrank on viettel comments\n",
    "================\n",
    "\n",
    "Dataset: 24K unique comments contain viettel, vietel,vt, etc.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- clustering the feeds\n",
    "- classification based on instances: kNN\n",
    "- prediction of cluster for each feed\n",
    "\n",
    "Steps:\n",
    "\n",
    "- how to represent feed: tfidf, lda, doc2vec 100?\n",
    "- how to measure similarity: cosine, jsd?\n",
    "- how to choose number of clusters?\n",
    "\n",
    "\n",
    "LR: 0.6, SVC: 0.4\n",
    "Mega Combinator:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        0.0       1.00      1.00      1.00     20377\n",
    "        1.0       0.98      0.98      0.98      3616\n",
    "\n",
    "avg / total       0.99      0.99      0.99     23993\n",
    "\n",
    "Mega Classifier: 0.99 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "STOPWORDS = [u\"ấy\", u\"bị\", u\"bởi\", u\"cả\", u\"các\", u\"cái\", u\"cần\", u\"càng\", u\"chỉ\", u\"chiếc\", u\"cho\", u\"chứ\", u\"chưa\", \n",
    "             u\"chuyện\", u\"có\", u\"có_thể\", u\"cứ\", u\"của\", u\"cùng\", u\"cũng\", u\"đã\", u\"đang\", u\"đây\", u\"để\", u\"đến_nỗi\", \n",
    "             u\"đều\", u\"điều\", u\"do\", u\"đó\", u\"được\", u\"dưới\", u\"gì\", u\"khi\", u\"không\", u\"là\", u\"lại\", u\"lên\", u\"lúc\", \n",
    "             u\"mà\", u\"mỗi\", u\"một cách\", u\"này\", u\"nên\", u\"nếu\", u\"ngay\", u\"nhiều\", u\"như\", u\"nhưng\", u\"những\", u\"nơi\", \n",
    "             u\"nữa\", u\"phải\", u\"qua\", u\"ra\", u\"rằng\", u\"rất\", u\"rồi\", u\"sau\", u\"sẽ\", u\"so\", u\"sự\", u\"tại\", u\"theo\", \n",
    "             u\"thì\", u\"trên\", u\"trước\", u\"từ\", u\"từng\", u\"và\", u\"vẫn\", u\"vào\", u\"vậy\", u\"vì\", u\"việc\", u\"với\", u\"vừa\",\n",
    "             u\"_num\", u\"wwdateww\", u\"wwtimeww\", u\"wwemailww\", u\"wwipww\", u\"wwurlww\", u\"wwnumberww\"\n",
    "            ]\n",
    "\n",
    "tbl = dict.fromkeys(i for i in xrange(sys.maxunicode)\n",
    "                      if unicodedata.category(unichr(i)).startswith('P') and i != 45 and i!= 95)\n",
    "\n",
    "def vi_trans_unicode(su):\n",
    "    return su.translate(tbl)\n",
    "\n",
    "def vi_strip_text2(s):\n",
    "    s = re.sub(r\"&amp;\", \"\", s)\n",
    "    s = vi_trans_unicode(s)\n",
    "    s = re.sub(r\"<([^>]+)>\", \"\", s)\n",
    "    s = re.sub(r\"(\\s|\\\\n|\\\\r|\\\\t)+\", \" \", s)\n",
    "    s = re.sub(r\"__+\", \"_\", s)\n",
    "    s = re.sub(r\"--+\", \"-\", s)\n",
    "    s = re.sub(r'(.)\\1+', r'\\1\\1', s)\n",
    "    s = ' '.join([w if not w[0].isdigit() else u\"wwNUMBERww\" for w in s.strip().split()])\n",
    "    return s\n",
    "\n",
    "\n",
    "def vi_clean3(line):\n",
    "    words = line.replace('.','').strip().split()\n",
    "    words = [w.lower() for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def vi_remove_stop_1char(line):\n",
    "    words = line.split()\n",
    "    words = [w for w in words if w not in STOPWORDS and len(w) > 1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "import requests\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "tok_url = \"http://192.168.0.215:8081/api/v1.0/document/filter\"\n",
    "\n",
    "def get_tokens(tok_url, data):\n",
    "    try:\n",
    "        rq = requests.post(tok_url, data=data.encode('utf-8'))\n",
    "        if rq.content:\n",
    "            tok_doc = ' '.join(json.loads(rq.content)['sentences'])\n",
    "        else:\n",
    "            tok_doc = None\n",
    "        return tok_doc\n",
    "    except Exception, e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/FE.feed.txt.nodup.word.tok3\", encoding=\"utf-8\", mode=\"r\") as fin:\n",
    "    feed_raw = fin.readlines()\n",
    "    \n",
    "print len(feed_raw)\n",
    "print feed_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_clean = []\n",
    "# pos_posts_io = []\n",
    "for i, raw in enumerate(feed_raw):\n",
    "    if i % 100 == 0: print \"[*] Progress \", i\n",
    "    doc = vi_remove_stop_1char(vi_clean3(vi_strip_text2(raw)))\n",
    "    feed_clean.append(doc)\n",
    "\n",
    "print len(feed_clean)\n",
    "print feed_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(min_df=2, \n",
    "                                   max_df=0.95,\n",
    "                                   sublinear_tf=True,\n",
    "                                   analyzer = \"word\",   \n",
    "                                   tokenizer = None,    \n",
    "                                   preprocessor = None, \n",
    "                                   stop_words = None,   \n",
    "                                   max_features = None\n",
    "                                )\n",
    "\n",
    "feed_feature_tfidf = vectorizer_tfidf.fit_transform(feed_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vectorizer_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print feed_feature_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_feature_tfidf = feed_feature_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = feed_feature_tfidf.toarray().copy()\n",
    "\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "PCA(copy=True, n_components=2, whiten=True)\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, n_jobs=4).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=5, batch_size=2000, n_init=10, max_no_improvement=100, verbose=0)\n",
    "mbk.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mbk.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbk_means_labels = mbk.labels_\n",
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "mbk_means_labels_unique = np.unique(mbk_means_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mbk_means_labels\n",
    "print\n",
    "print mbk_means_cluster_centers\n",
    "print\n",
    "print mbk_means_labels_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbk2 = MiniBatchKMeans(init='k-means++', n_clusters=20, batch_size=2000, n_init=10, max_no_improvement=100, verbose=0)\n",
    "mbk2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mbk2.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbk50 = MiniBatchKMeans(init='k-means++', n_clusters=50, batch_size=2000, n_init=10, max_no_improvement=100, verbose=0)\n",
    "mbk50.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mbk50.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(mbk, \"/home/laampt/nlp/feedrank/model/feedrank_24Kdoc_mbk5.pkl\")\n",
    "joblib.dump(mbk2, \"/home/laampt/nlp/feedrank/model/feedrank_24Kdoc_mbk20.pkl\")\n",
    "joblib.dump(mbk50, \"/home/laampt/nlp/feedrank/model/feedrank_24Kdoc_mbk50.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer_tfidf, \"/home/laampt/nlp/feedrank/model/feedrank_24Kdoc_vectorizer_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_test = u\"lào quan_trọng hơn việt_nam viettel đang lo cho nước bạn hơn còn trong nước cứ củ_từ chán vãi\"\n",
    "s_test_clean = vi_remove_stop_1char(vi_clean3(vi_strip_text2(s_test)))\n",
    "\n",
    "print s_test\n",
    "print \"CLEAN: \" + '-' * 80\n",
    "print s_test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs = vectorizer_tfidf.transform([s_test_clean])\n",
    "vec5 = mbk.transform(vecs)\n",
    "vec20 = mbk2.transform(vecs)\n",
    "vec50 = mbk50.transform(vecs)\n",
    "\n",
    "print vecs[0]\n",
    "print \"Kmean 5: \" + '-' * 80\n",
    "print vec5[0]\n",
    "print \"Kmean 20: \" + '-' * 80\n",
    "print vec20[0]\n",
    "print \"Kmean 50: \" + '-' * 80\n",
    "print vec50[0]\n",
    "\n",
    "print '-' * 80\n",
    "print \"K5: {}, K20: {}, K50: {}\".format(mbk.predict(vecs)[0], mbk2.predict(vecs)[0], mbk50.predict(vecs)[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN distance weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=11, weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " print(\"Top terms per cluster:\")\n",
    "order_centroids = mbk2.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer_tfidf.get_feature_names()\n",
    "for i in range(20):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print '.' * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbk2.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vec20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.argsort(vec20[0])\n",
    "print 'REVERSED: ' + '-' * 80\n",
    "print np.argsort(vec20[0])[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/good_feeds.txt\", encoding=\"utf-8\", mode=\"r\") as fin:\n",
    "    for line in fin:\n",
    "        s_test_clean = vi_remove_stop_1char(vi_clean3(vi_strip_text2(line)))\n",
    "        vecs = vectorizer_tfidf.transform([s_test_clean])\n",
    "        prediction2 = mbk2.predict(vecs)[0]\n",
    "        prediction50 = mbk50.predict(vecs)[0]\n",
    "        if prediction: predictions.append(prediction)\n",
    "        print s_test_clean\n",
    "        print prediction2, prediction50\n",
    "        print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ii, label in enumerate(mbk2.labels_):\n",
    "    print feed_clean[ii], label\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "from tsne import tsne # See http://lvdmaaten.github.io/tsne/#implementations\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "\n",
    "plt.style.use('fivethirtyeight') # Good looking plots\n",
    "pd.set_option('display.max_columns', None) # Display any number of columns\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df = pd.DataFrame(feed_clean, columns=['Feed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df['C5'] = mbk.labels_\n",
    "feed_df['C20'] = mbk2.labels_\n",
    "feed_df['C50'] = mbk50.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df.hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feed_df[feed_df.C20 == 10][feed_df.C50 == 35].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df[feed_df.C20 == 11].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print feed_df.ix[13]['Feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df[feed_df.C20 == 16].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_df[feed_df.C20 == 12][feed_df.C50 == 46].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mega predict\n",
    "\n",
    "$$ predict(s) = mbk2.predict(s) + mbk50.predict(s) $$\n",
    "\n",
    "prediction = 1 if:\n",
    "\n",
    "- c20 == 10 & c50 == 35 \n",
    "- c20 == 11 & c50 == 31\n",
    "- c20 == 12 & c50 == 32\n",
    "- c20 == 16 & c50 == 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mbk_prediction(feed, vectorizer_tfidf, mbk2, mbk50):\n",
    "    \"\"\"\n",
    "    Input: given raw feed, vectorizer, kmean 20 clusters, kmean 50 clusters\n",
    "    Output: 1 if feed is user opinion alike else 0\n",
    "    \"\"\"\n",
    "    s_test_clean = vi_remove_stop_1char(vi_clean3(vi_strip_text2(feed)))\n",
    "    vecs = vectorizer_tfidf.transform([s_test_clean])\n",
    "    p20 = mbk2.predict(vecs)[0]\n",
    "    p50 = mbk50.predict(vecs)[0]\n",
    "    \n",
    "    if (p20 == 10 and p50 == 35) or (p20 == 11 and p50 == 31) or (p20 == 12 and p50 == 32) or (p20 == 16 and p50 == 9):\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed = u\"minh gooner chuyển xài viettel gói_cước đi thấy xài good nhá\"\n",
    "s_test_clean = vi_remove_stop_1char(vi_clean3(vi_strip_text2(feed)))\n",
    "print s_test_clean\n",
    "vecs = vectorizer_tfidf.transform([s_test_clean])\n",
    "print vecs[0]\n",
    "print mbk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/good_feeds.txt\", encoding=\"utf-8\", mode=\"r\") as fin:\n",
    "    for line in fin:\n",
    "        prediction = mega_prediction(line, vectorizer_tfidf, mbk2, mbk50)\n",
    "        \n",
    "        print line\n",
    "        print 'RESULT: [{}]'.format(prediction)\n",
    "        print '.' * 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_feeds = []\n",
    "\n",
    "sent_feeds += list(feed_df[feed_df.C20 == 10][feed_df.C50 == 35]['Feed'].values)\n",
    "sent_feeds += list(feed_df[feed_df.C20 == 11][feed_df.C50 == 31]['Feed'].values)\n",
    "sent_feeds += list(feed_df[feed_df.C20 == 12][feed_df.C50 == 32]['Feed'].values)\n",
    "sent_feeds += list(feed_df[feed_df.C20 == 16][feed_df.C50 == 9]['Feed'].values)\n",
    "\n",
    "print len(sent_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sent_feeds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/sent_feeds.txt\", encoding='utf-8', mode='w') as fout:\n",
    "    for feed in sent_feeds:\n",
    "        fout.write(feed + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/fact_feeds.txt\", encoding='utf-8', mode='w') as fout:\n",
    "    for feed in feed_clean:\n",
    "        if feed:\n",
    "            prediction = mega_prediction(feed, vectorizer_tfidf, mbk2, mbk50)\n",
    "            if prediction == 0:\n",
    "                fout.write(feed + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3616\n"
     ]
    }
   ],
   "source": [
    "pos_docs = []\n",
    "\n",
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/sent_feeds.txt.nodup\", encoding=\"utf-8\", mode=\"r\") as fin:\n",
    "    pos_docs = fin.readlines()\n",
    "    \n",
    "print len(pos_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20377\n"
     ]
    }
   ],
   "source": [
    "neg_docs = []\n",
    "\n",
    "with codecs.open(\"/home/laampt/nlp/data/feeds/tok/fact_feeds.txt.nodup\", encoding=\"utf-8\", mode=\"r\") as fin:\n",
    "    neg_docs = fin.readlines()\n",
    "    \n",
    "print len(neg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Progress  0\n",
      "[*] Progress  100\n",
      "[*] Progress  200\n",
      "[*] Progress  300\n",
      "[*] Progress  400\n",
      "[*] Progress  500\n",
      "[*] Progress  600\n",
      "[*] Progress  700\n",
      "[*] Progress  800\n",
      "[*] Progress  900\n",
      "[*] Progress  1000\n",
      "[*] Progress  1100\n",
      "[*] Progress  1200\n",
      "[*] Progress  1300\n",
      "[*] Progress  1400\n",
      "[*] Progress  1500\n",
      "[*] Progress  1600\n",
      "[*] Progress  1700\n",
      "[*] Progress  1800\n",
      "[*] Progress  1900\n",
      "[*] Progress  2000\n",
      "[*] Progress  2100\n",
      "[*] Progress  2200\n",
      "[*] Progress  2300\n",
      "[*] Progress  2400\n",
      "[*] Progress  2500\n",
      "[*] Progress  2600\n",
      "[*] Progress  2700\n",
      "[*] Progress  2800\n",
      "[*] Progress  2900\n",
      "[*] Progress  3000\n",
      "[*] Progress  3100\n",
      "[*] Progress  3200\n",
      "[*] Progress  3300\n",
      "[*] Progress  3400\n",
      "[*] Progress  3500\n",
      "[*] Progress  3600\n"
     ]
    }
   ],
   "source": [
    "pos_docs_clean_io = []\n",
    "for i, raw in enumerate(pos_docs):\n",
    "    if i % 100 == 0: print \"[*] Progress \", i\n",
    "    doc = vi_remove_stop_1char(vi_clean3(vi_strip_text2(raw)))\n",
    "    pos_docs_clean_io.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Progress  0\n",
      "[*] Progress  100\n",
      "[*] Progress  200\n",
      "[*] Progress  300\n",
      "[*] Progress  400\n",
      "[*] Progress  500\n",
      "[*] Progress  600\n",
      "[*] Progress  700\n",
      "[*] Progress  800\n",
      "[*] Progress  900\n",
      "[*] Progress  1000\n",
      "[*] Progress  1100\n",
      "[*] Progress  1200\n",
      "[*] Progress  1300\n",
      "[*] Progress  1400\n",
      "[*] Progress  1500\n",
      "[*] Progress  1600\n",
      "[*] Progress  1700\n",
      "[*] Progress  1800\n",
      "[*] Progress  1900\n",
      "[*] Progress  2000\n",
      "[*] Progress  2100\n",
      "[*] Progress  2200\n",
      "[*] Progress  2300\n",
      "[*] Progress  2400\n",
      "[*] Progress  2500\n",
      "[*] Progress  2600\n",
      "[*] Progress  2700\n",
      "[*] Progress  2800\n",
      "[*] Progress  2900\n",
      "[*] Progress  3000\n",
      "[*] Progress  3100\n",
      "[*] Progress  3200\n",
      "[*] Progress  3300\n",
      "[*] Progress  3400\n",
      "[*] Progress  3500\n",
      "[*] Progress  3600\n",
      "[*] Progress  3700\n",
      "[*] Progress  3800\n",
      "[*] Progress  3900\n",
      "[*] Progress  4000\n",
      "[*] Progress  4100\n",
      "[*] Progress  4200\n",
      "[*] Progress  4300\n",
      "[*] Progress  4400\n",
      "[*] Progress  4500\n",
      "[*] Progress  4600\n",
      "[*] Progress  4700\n",
      "[*] Progress  4800\n",
      "[*] Progress  4900\n",
      "[*] Progress  5000\n",
      "[*] Progress  5100\n",
      "[*] Progress  5200\n",
      "[*] Progress  5300\n",
      "[*] Progress  5400\n",
      "[*] Progress  5500\n",
      "[*] Progress  5600\n",
      "[*] Progress  5700\n",
      "[*] Progress  5800\n",
      "[*] Progress  5900\n",
      "[*] Progress  6000\n",
      "[*] Progress  6100\n",
      "[*] Progress  6200\n",
      "[*] Progress  6300\n",
      "[*] Progress  6400\n",
      "[*] Progress  6500\n",
      "[*] Progress  6600\n",
      "[*] Progress  6700\n",
      "[*] Progress  6800\n",
      "[*] Progress  6900\n",
      "[*] Progress  7000\n",
      "[*] Progress  7100\n",
      "[*] Progress  7200\n",
      "[*] Progress  7300\n",
      "[*] Progress  7400\n",
      "[*] Progress  7500\n",
      "[*] Progress  7600\n",
      "[*] Progress  7700\n",
      "[*] Progress  7800\n",
      "[*] Progress  7900\n",
      "[*] Progress  8000\n",
      "[*] Progress  8100\n",
      "[*] Progress  8200\n",
      "[*] Progress  8300\n",
      "[*] Progress  8400\n",
      "[*] Progress  8500\n",
      "[*] Progress  8600\n",
      "[*] Progress  8700\n",
      "[*] Progress  8800\n",
      "[*] Progress  8900\n",
      "[*] Progress  9000\n",
      "[*] Progress  9100\n",
      "[*] Progress  9200\n",
      "[*] Progress  9300\n",
      "[*] Progress  9400\n",
      "[*] Progress  9500\n",
      "[*] Progress  9600\n",
      "[*] Progress  9700\n",
      "[*] Progress  9800\n",
      "[*] Progress  9900\n",
      "[*] Progress  10000\n",
      "[*] Progress  10100\n",
      "[*] Progress  10200\n",
      "[*] Progress  10300\n",
      "[*] Progress  10400\n",
      "[*] Progress  10500\n",
      "[*] Progress  10600\n",
      "[*] Progress  10700\n",
      "[*] Progress  10800\n",
      "[*] Progress  10900\n",
      "[*] Progress  11000\n",
      "[*] Progress  11100\n",
      "[*] Progress  11200\n",
      "[*] Progress  11300\n",
      "[*] Progress  11400\n",
      "[*] Progress  11500\n",
      "[*] Progress  11600\n",
      "[*] Progress  11700\n",
      "[*] Progress  11800\n",
      "[*] Progress  11900\n",
      "[*] Progress  12000\n",
      "[*] Progress  12100\n",
      "[*] Progress  12200\n",
      "[*] Progress  12300\n",
      "[*] Progress  12400\n",
      "[*] Progress  12500\n",
      "[*] Progress  12600\n",
      "[*] Progress  12700\n",
      "[*] Progress  12800\n",
      "[*] Progress  12900\n",
      "[*] Progress  13000\n",
      "[*] Progress  13100\n",
      "[*] Progress  13200\n",
      "[*] Progress  13300\n",
      "[*] Progress  13400\n",
      "[*] Progress  13500\n",
      "[*] Progress  13600\n",
      "[*] Progress  13700\n",
      "[*] Progress  13800\n",
      "[*] Progress  13900\n",
      "[*] Progress  14000\n",
      "[*] Progress  14100\n",
      "[*] Progress  14200\n",
      "[*] Progress  14300\n",
      "[*] Progress  14400\n",
      "[*] Progress  14500\n",
      "[*] Progress  14600\n",
      "[*] Progress  14700\n",
      "[*] Progress  14800\n",
      "[*] Progress  14900\n",
      "[*] Progress  15000\n",
      "[*] Progress  15100\n",
      "[*] Progress  15200\n",
      "[*] Progress  15300\n",
      "[*] Progress  15400\n",
      "[*] Progress  15500\n",
      "[*] Progress  15600\n",
      "[*] Progress  15700\n",
      "[*] Progress  15800\n",
      "[*] Progress  15900\n",
      "[*] Progress  16000\n",
      "[*] Progress  16100\n",
      "[*] Progress  16200\n",
      "[*] Progress  16300\n",
      "[*] Progress  16400\n",
      "[*] Progress  16500\n",
      "[*] Progress  16600\n",
      "[*] Progress  16700\n",
      "[*] Progress  16800\n",
      "[*] Progress  16900\n",
      "[*] Progress  17000\n",
      "[*] Progress  17100\n",
      "[*] Progress  17200\n",
      "[*] Progress  17300\n",
      "[*] Progress  17400\n",
      "[*] Progress  17500\n",
      "[*] Progress  17600\n",
      "[*] Progress  17700\n",
      "[*] Progress  17800\n",
      "[*] Progress  17900\n",
      "[*] Progress  18000\n",
      "[*] Progress  18100\n",
      "[*] Progress  18200\n",
      "[*] Progress  18300\n",
      "[*] Progress  18400\n",
      "[*] Progress  18500\n",
      "[*] Progress  18600\n",
      "[*] Progress  18700\n",
      "[*] Progress  18800\n",
      "[*] Progress  18900\n",
      "[*] Progress  19000\n",
      "[*] Progress  19100\n",
      "[*] Progress  19200\n",
      "[*] Progress  19300\n",
      "[*] Progress  19400\n",
      "[*] Progress  19500\n",
      "[*] Progress  19600\n",
      "[*] Progress  19700\n",
      "[*] Progress  19800\n",
      "[*] Progress  19900\n",
      "[*] Progress  20000\n",
      "[*] Progress  20100\n",
      "[*] Progress  20200\n",
      "[*] Progress  20300\n"
     ]
    }
   ],
   "source": [
    "neg_docs_clean_io = []\n",
    "\n",
    "for i, raw in enumerate(neg_docs):\n",
    "    if i % 100 == 0: print \"[*] Progress \", i\n",
    "    doc = vi_remove_stop_1char(vi_clean3(vi_strip_text2(raw)))\n",
    "    neg_docs_clean_io.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tfidf = joblib.load(\"/home/laampt/nlp/feedrank/model/feedrank_24Kdoc_vectorizer_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs  = pos_docs_clean_io + neg_docs_clean_io\n",
    "train_docs_list = [doc.split() for doc in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_docs)\n",
    "y_train = np.hstack((np.ones(len(pos_docs_clean_io)), np.zeros(len(neg_docs_clean_io))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features3 = vectorizer_tfidf.transform(train_docs)\n",
    "train_data_features3 = train_data_features3.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratified_cv3(X, y, clf, shuffle=True, n_folds=10, **kwargs):\n",
    "    stratified_k_fold = cross_validation.StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)\n",
    "    y_pred = y.copy()\n",
    "    for ii, jj in stratified_k_fold:\n",
    "        X_train, X_test = X[ii], X[jj]\n",
    "        y_train = y[ii]\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[jj] = clf.predict(X_test)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_tfidf10 = stratified_cv3(train_data_features3, y_train, linear_model.LogisticRegression(C=100))\n",
    "svc_tfidf10 = stratified_cv3(train_data_features3, y_train, svm.LinearSVC(C=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     20377\n",
      "        1.0       0.98      0.98      0.98      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Support vector machine(SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99     20377\n",
      "        1.0       0.97      0.96      0.96      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Logistic Regression:\\n{}'.format(metrics.classification_report(y_train, lr_tfidf10.predict(train_data_features3)))\n",
    "print 'Support vector machine(SVM):\\n {}'.format(metrics.classification_report(y_train, svc_tfidf10.predict(train_data_features3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.1, SVC: 0.9\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99     20377\n",
      "        1.0       0.97      0.96      0.96      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.98 AUC\n",
      "\n",
      "LR: 0.2, SVC: 0.8\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99     20377\n",
      "        1.0       0.97      0.96      0.96      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.98 AUC\n",
      "\n",
      "LR: 0.3, SVC: 0.7\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99     20377\n",
      "        1.0       0.97      0.96      0.96      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.98 AUC\n",
      "\n",
      "LR: 0.4, SVC: 0.6\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99     20377\n",
      "        1.0       0.97      0.96      0.96      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.98 AUC\n",
      "\n",
      "LR: 0.5, SVC: 0.5\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      0.99     20377\n",
      "        1.0       0.99      0.94      0.96      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.97 AUC\n",
      "\n",
      "LR: 0.6, SVC: 0.4\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     20377\n",
      "        1.0       0.98      0.98      0.98      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.99 AUC\n",
      "\n",
      "LR: 0.7, SVC: 0.3\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     20377\n",
      "        1.0       0.98      0.98      0.98      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.99 AUC\n",
      "\n",
      "LR: 0.8, SVC: 0.2\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     20377\n",
      "        1.0       0.98      0.98      0.98      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.99 AUC\n",
      "\n",
      "LR: 0.9, SVC: 0.1\n",
      "Mega Combinator:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     20377\n",
      "        1.0       0.98      0.98      0.98      3616\n",
      "\n",
      "avg / total       0.99      0.99      0.99     23993\n",
      "\n",
      "Mega Classifier: 0.99 AUC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_weights = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "mega_predictions = []\n",
    "for w in lr_weights:\n",
    "    print \"LR: {}, SVC: {}\".format(w,1-w)\n",
    "    mega_predictions = []\n",
    "    for vec in train_data_features3:\n",
    "        prediction = lr_tfidf10.predict(vec) * w + svc_tfidf10.predict(vec) * (1-w)\n",
    "        prediction = 1 if (prediction[0] > 0.5) else 0\n",
    "        mega_predictions.append(prediction)\n",
    "    print 'Mega Combinator:\\n{}'.format(metrics.classification_report(y_train, mega_predictions))\n",
    "    print('Mega Classifier: {:.2f} AUC'.format(metrics.roc_auc_score(y_train, mega_predictions)))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/laampt/nlp/feedrank/estimator/39/svc_tfidf_99auc_10cv2.pkl',\n",
       " '/home/laampt/nlp/feedrank/estimator/39/svc_tfidf_99auc_10cv2.pkl_01.npy',\n",
       " '/home/laampt/nlp/feedrank/estimator/39/svc_tfidf_99auc_10cv2.pkl_02.npy',\n",
       " '/home/laampt/nlp/feedrank/estimator/39/svc_tfidf_99auc_10cv2.pkl_03.npy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lr_tfidf10, \"/home/laampt/nlp/feedrank/estimator/39/lr_tfidf_99auc_10cv2.pkl\")\n",
    "joblib.dump(svc_tfidf10, \"/home/laampt/nlp/feedrank/estimator/39/svc_tfidf_99auc_10cv2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mega_prediction2(feed, vectorizer_tfidf, lr_tfidf10, svc_tfidf10):\n",
    "    \"\"\"\n",
    "    Input: given raw feed, vectorizer, lr, svm classifier\n",
    "    Output: 1 if feed is user opinion alike else 0\n",
    "    \"\"\"\n",
    "    s_test_clean = vi_remove_stop_1char(vi_clean3(vi_strip_text2(feed)))\n",
    "    vec = vectorizer_tfidf.transform([s_test_clean])[0]\n",
    "    prediction = lr_tfidf10.predict(vec) * w + svc_tfidf10.predict(vec) * (1-w)\n",
    "    prediction = 1 if (prediction[0] > 0.5) else 0\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos1 = u\"vt làm_việc chuyên_nghiệp và nhiệt_tình phết _wwNUMBERww_ mình tới đây lần nào cũng hài_lòng\"\n",
    "neg2 = u\"chương_trình khuyến_mãi thẻ_cào nhân dịp quốc_khánh wwDATEww Nạp wwNUMBERww được wwNUMBERww ông chú vt bảo vậy\"\n",
    "\n",
    "pred_mbk_pos1 = mbk_prediction(pos1, vectorizer_tfidf, mbk2, mbk50)\n",
    "pred_mbk_neg2 = mbk_prediction(neg2, vectorizer_tfidf, mbk2, mbk50)\n",
    "\n",
    "pred_mega2_pos1 = mega_prediction2(pos1, vectorizer_tfidf, lr_tfidf10, svc_tfidf10)\n",
    "pred_mega2_neg2 = mega_prediction2(neg2, vectorizer_tfidf, lr_tfidf10, svc_tfidf10)\n",
    "\n",
    "print \"REPORT\"\n",
    "print \"-\" * 80\n",
    "\n",
    "print \"POS: \"\n",
    "print pos1\n",
    "print \"PREDICT: MBK [{}] MEGA [{}]\".format(pred_mbk_pos1, pred_mega2_pos1)\n",
    "print \".\" * 80\n",
    "\n",
    "print \"NEG: \"\n",
    "print neg2\n",
    "print \"PREDICT: MBK [{}] MEGA [{}]\".format(pred_mbk_neg2, pred_mega2_neg2)\n",
    "print \".\" * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
